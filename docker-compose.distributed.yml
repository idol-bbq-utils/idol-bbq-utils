# Distributed deployment for idol-bbq-utils
# This configuration splits the monolithic tweet-forwarder into separate services:
# - scheduler-service: Schedules spider and forwarder tasks via cron + handles forwarding workers
# - crawler-service: Handles web scraping with browser pool + integrated storage (翻译 + 存储)
#
# scheduler-service 运行模式 (通过环境变量控制):
#   1. 混合模式 (默认): ENABLE_SCHEDULER=true + ENABLE_SENDER_WORKER=true
#      同时运行任务调度器和转发 worker
#   2. 纯调度模式: ENABLE_SCHEDULER=true + ENABLE_SENDER_WORKER=false
#      仅运行任务调度器，不执行转发（需要单独部署 forwarder worker）
#   3. 纯 Worker 模式: ENABLE_SCHEDULER=false + ENABLE_SENDER_WORKER=true
#      仅运行转发 worker，不调度任务（适合横向扩展 worker）
#
# Usage:
#   docker-compose -f docker-compose.distributed.yml up -d
#   docker-compose -f docker-compose.distributed.yml --profile scheduler up -d
#

services:
  redis:
    image: redis:7-alpine
    container_name: idol-bbq-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Bull Board - Queue monitoring dashboard
  bull-board:
    image: deadly0/bull-board:latest
    container_name: idol-bbq-bull-board
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_USE_TLS: "false"
      BULL_PREFIX: "bull"
    depends_on:
      redis:
        condition: service_healthy

  scheduler:
    build:
      context: .
      dockerfile: app/scheduler-service/Dockerfile
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - LOG_LEVEL=info
      - TZ=Asia/Tokyo
      - DATABASE_URL=file:/app/data/data.db
      - ENABLE_SCHEDULER=true
      - ENABLE_SENDER_WORKER=true
      - SENDER_WORKER_CONCURRENCY=2
      - CACHE_DIR=/app/data/tmp
    volumes:
      - ./data:/app/data
      - ./data/config.yaml:/app/config.yaml:ro
    depends_on:
      redis:
        condition: service_healthy

  # Crawler Service - Web scraping workers with browser pool + integrated storage
  crawler:
    build:
      context: .
      dockerfile: app/crawler-service/Dockerfile
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    environment:
      - TZ=Asia/Tokyo
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - WORKER_CONCURRENCY=6
      - BROWSER_POOL_SIZE=1
      - STORAGE_QUEUE_CONCURRENCY=3
      - NO_SANDBOX=true
      - LOG_LEVEL=info
      - DATABASE_URL=file:/app/data/data.db
    volumes:
      - ./data:/app/data
    depends_on:
      redis:
        condition: service_healthy


volumes:
  redis_data:
